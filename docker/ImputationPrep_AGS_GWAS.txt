

#	Start instance

create_ec2_instance.bash --profile gwendt --image-id ami-0323c3dd2da7fb37d --instance-type i3.2xlarge --key-name ~/.aws/JakeHervUNR.pem --NOT-DRY-RUN

ssh ...


#	Update instance

sudo yum update
sudo yum install docker htop



#	Prepare attached ssd

lsblk
sudo mkfs -t xfs /dev/nvme0n1
mkdir -p /ssd0/data
mkdir -p /ssd0/docker
sudo mount /dev/nvme0n1 /ssd0/
sudo chown ec2-user /ssd0/


#	Download data

aws s3 sync s3://herv-unr/20200520_Adult_Glioma_Study_GWAS/ /ssd0/data/



#	Setup docker

sudo bash -c 'echo { \"data-root\":\"/ssd0/docker/\" } >> /etc/docker/daemon.json'
sudo service docker start
sudo usermod -a -G docker ec2-user


#	Exit and reconnect ( apparently for the usermod permissions to docker )

exit
ssh ...



#	Get docker file

mkdir ~/tmp/
cd ~/tmp/
wget https://raw.githubusercontent.com/ucsffrancislab/genomics/master/docker/ImputationPrep.Dockerfile



#	Build it
docker build -t impute --file docker/ImputationPrep.Dockerfile ./

#	Run it in the background
docker run -v /ssd0/data:/data -itd impute

#	Connect to it
docker exec -it $( docker ps -aq ) bash

# Convert ped/map to bed


#	Create a frequency file

plink --freq --bfile /data/il370_4677 --out /data/il370_4677.freq
plink --freq --bfile /data/onco_1347  --out /data/onco_1347.freq


#	Execute script ( failed on my laptop as ran out of memory (15GB) )

perl HRC-1000G-check-bim.pl -b /data/il370_4677.bim -f /data/il370_4677.freq.frq -r HRC.r1-1.GRCh37.wgs.mac5.sites.tab -h
sh Run-plink.sh

perl HRC-1000G-check-bim.pl -b /data/onco_1347.bim -f /data/onco_1347.freq.frq -r HRC.r1-1.GRCh37.wgs.mac5.sites.tab -h
sh Run-plink.sh






#	Not sure why had to do anything since had the bim at the beginning?
#	Which reference? Does it really matter? Probably needs matching chomosome names?

#	Create vcf using [VcfCooker](http://genome.sph.umich.edu/wiki/VcfCooker)

vcfCooker --in-bfile /data/il370_4677.bim --ref <reference.fasta>  --out <output-vcf> --write-vcf
bgzip <output-vcf>
vcfCooker --in-bfile /data/onco_1347.bim --ref <reference.fasta>  --out <output-vcf> --write-vcf
bgzip <output-vcf>























#	Convert ped/map files to VCF files

Several tools are available: [plink2](https://www.cog-genomics.org/plink2/), [BCFtools](https://samtools.github.io/bcftools) or [VcfCooker](http://genome.sph.umich.edu/wiki/VcfCooker).
```
plink --ped study_chr1.ped --map study_chr1.map --recode vcf --out study_chr1
```

Create a sorted vcf.gz file using [BCFtools](https://samtools.github.io/bcftools):
```
bcftools sort study_chr1.vcf -Oz -o study_chr1.vcf.gz
```

#	CheckVCF

Use [checkVCF](https://github.com/zhanxw/checkVCF) to ensure that the VCF files are valid. checkVCF proposes "Action Items" (e.g. upload to sftp server), which can be ignored. Only the validity should be checked with this command.
```
checkVCF.py -r human_g1k_v37.fasta -o out mystudy_chr1.vcf.gz
```














#	Assuming that this is the ONLY container, stop it and remove it when done.
#	Of course, if you are about to shutdown the AWS instance, its irrelevant.

docker stop $( docker ps -aq )

docker rm $( docker ps -aq )

