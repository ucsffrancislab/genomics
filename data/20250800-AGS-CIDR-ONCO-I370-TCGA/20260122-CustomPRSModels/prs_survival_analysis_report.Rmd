#!/usr/bin/env Rscript

args <- commandArgs()
fname <- normalizePath(sub("--file=", "", args[grepl("--file=", args)]))
thisfile <- readLines(fname)
newfname <- paste0(tempdir(), "/", basename(fname))
writeLines(thisfile[-1:-which(thisfile == "q(\"no\")")], newfname)

args = commandArgs(trailingOnly=TRUE)
output_file = paste(basename(fname),"html", sep=".")
print(output_file)

rmarkdown::render(newfname, output_dir = dirname(fname), output_file = output_file )
q("no")


---
title: "Polygenic Risk Score Survival Analysis in Glioma"
author: "Comprehensive Analysis Report"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true
    theme: united
    highlight: tango
    code_folding: hide
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 8,
  dpi = 150
)

# Load required libraries
library(tidyverse)
library(data.table)
library(survival)
library(survminer)
library(ggpubr)
library(corrplot)
library(pheatmap)
library(scales)
library(RColorBrewer)
library(gridExtra)
library(knitr)
library(DT)

# Set theme
theme_set(theme_bw(base_size = 12))

# Color palettes
dataset_colors <- c("cidr" = "#E41A1C", "onco" = "#377EB8", 
                    "i370" = "#4DAF4A", "tcga" = "#984EA3")
model_colors <- colorRampPalette(brewer.pal(8, "Set2"))
```

# Executive Summary

This report documents the comprehensive analysis of Polygenic Risk Scores (PRS) in glioma survival across four independent cohorts:

- **CIDR**: Case-control study
- **ONCO**: Oncology cohort  
- **I370**: Independent validation cohort
- **TCGA**: The Cancer Genome Atlas

We analyzed **5,117 PRS models** including:
- ~5,000 models from the PGS Catalog
- **7 custom glioma-specific models** (3 with methodology comparison)

**Key objectives:**
1. Quality control of PRS calculations
2. Validation of multiallelic variant splitting approach
3. Cox proportional hazards survival analysis per cohort
4. Fixed-effects meta-analysis across cohorts
5. Identification of PRS associated with glioma survival

---

# Data Loading and Preprocessing

## Raw PRS Scores

```{r load_raw_scores}
# Base path
base_path <- "/francislab/data1/working/20250800-AGS-CIDR-ONCO-I370-TCGA/20260122-CustomPRSModels/pgs-calc-scores-new_models"

datasets <- c("cidr", "onco", "i370", "tcga")

# Load raw scores for all datasets
raw_scores_list <- lapply(datasets, function(ds) {
  file_path <- file.path(base_path, ds, "scores.txt")
  if(file.exists(file_path)) {
    df <- fread(file_path)
    # Strip quotes
    names(df) <- gsub('^"|"$', '', names(df))
    df$sample <- gsub('^"|"$', '', df$sample)
    df$dataset <- ds
    return(df)
  } else {
    cat("Warning: File not found:", file_path, "\n")
    return(NULL)
  }
})

names(raw_scores_list) <- datasets

# Combine for summary statistics
cat("Successfully loaded", sum(sapply(raw_scores_list, function(x) !is.null(x))), "datasets\n")

# Sample sizes
sample_sizes <- sapply(raw_scores_list, function(x) if(!is.null(x)) nrow(x) else 0)
kable(data.frame(Dataset = datasets, N_samples = sample_sizes),
      caption = "Sample sizes per dataset")
```

## Z-scored PRS

```{r load_zscores}
# Load z-scored versions
zscores_list <- lapply(datasets, function(ds) {
  file_path <- file.path(base_path, ds, "scores.z-scores.txt")
  if(file.exists(file_path)) {
    df <- fread(file_path)
    names(df) <- gsub('^"|"$', '', names(df))
    df$sample <- gsub('^"|"$', '', df$sample)
    df$dataset <- ds
    return(df)
  } else {
    return(NULL)
  }
})

names(zscores_list) <- datasets
```

## Model Categories

```{r identify_models}
# Get all PRS model names (excluding sample and dataset columns)
all_models <- setdiff(names(raw_scores_list[[1]]), c("sample", "dataset"))

# Identify custom models
custom_models <- grep("glioma|gbm|idh", all_models, value = TRUE, ignore.case = TRUE)

# Identify the comma vs split versions
comma_models <- grep("\\.commas$", custom_models, value = TRUE)
split_models <- gsub("\\.commas$", "", comma_models)

cat("Total PRS models:", length(all_models), "\n")
cat("Custom glioma models:", length(custom_models), "\n")
cat("  - Comma versions:", length(comma_models), "\n")
cat("  - Split versions:", length(split_models), "\n")
cat("PGS Catalog models:", length(all_models) - length(custom_models), "\n")

# Create summary table
model_summary <- data.frame(
  Category = c("PGS Catalog", "Custom - Comma", "Custom - Split", "Total"),
  Count = c(length(all_models) - length(custom_models), 
            length(comma_models), 
            length(split_models),
            length(all_models)),
  Description = c("Standard catalog models",
                  "Custom models with multiallelic commas",
                  "Custom models with split variants",
                  "All models")
)

kable(model_summary, caption = "PRS Model Categories")
```

---

# Quality Control: PRS Score Distributions

## Raw Score Distributions by Dataset

```{r raw_score_distributions, fig.height=10}
# Sample 20 random PRS for visualization
set.seed(42)
sample_prs <- sample(setdiff(all_models, c(comma_models, split_models)), 
                     min(20, length(all_models)))

# Combine datasets
plot_data <- bind_rows(lapply(datasets, function(ds) {
  if(!is.null(raw_scores_list[[ds]])) {
    raw_scores_list[[ds]] %>%
      select(sample, dataset, all_of(sample_prs)) %>%
      pivot_longer(cols = all_of(sample_prs), 
                   names_to = "PRS", 
                   values_to = "Score")
  }
}))

# Plot distributions
ggplot(plot_data, aes(x = Score, fill = dataset)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ PRS, scales = "free", ncol = 4) +
  scale_fill_manual(values = dataset_colors) +
  labs(title = "Raw PRS Score Distributions (Sample of 20 Models)",
       subtitle = "Showing variation in scale across different PRS models",
       x = "Raw PRS Score", y = "Density") +
  theme(legend.position = "bottom")
```

**Observation:** Raw PRS scores have wildly different scales, confirming the need for standardization.

## Z-score Validation

```{r zscore_validation}
# Check z-scoring for one dataset (CIDR)
if(!is.null(zscores_list[["cidr"]])) {
  zscore_stats <- zscores_list[["cidr"]] %>%
    select(-sample, -dataset) %>%
    summarise(across(everything(), list(
      mean = ~mean(., na.rm = TRUE),
      sd = ~sd(., na.rm = TRUE)
    )))
  
  # Reshape for plotting
  zscore_check <- data.frame(
    PRS = gsub("_mean$|_sd$", "", names(zscore_stats)),
    Statistic = rep(c("Mean", "SD"), each = ncol(zscore_stats)/2),
    Value = as.numeric(zscore_stats)
  ) %>%
    filter(Statistic == "Mean" | Statistic == "SD")
  
  # Split into mean and SD
  means <- zscore_check %>% filter(Statistic == "Mean")
  sds <- zscore_check %>% filter(Statistic == "SD")
  
  # Plot mean distribution
  p1 <- ggplot(means, aes(x = Value)) +
    geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
    geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 1) +
    labs(title = "Distribution of PRS Means After Z-scoring (CIDR)",
         subtitle = "Should be centered at 0",
         x = "Mean", y = "Number of PRS models") +
    annotate("text", x = 0.5, y = Inf, vjust = 2, 
             label = paste0("Median = ", round(median(means$Value), 4)))
  
  # Plot SD distribution  
  p2 <- ggplot(sds, aes(x = Value)) +
    geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.7) +
    geom_vline(xintercept = 1, color = "red", linetype = "dashed", size = 1) +
    labs(title = "Distribution of PRS Standard Deviations After Z-scoring (CIDR)",
         subtitle = "Should be centered at 1",
         x = "Standard Deviation", y = "Number of PRS models") +
    annotate("text", x = 1.5, y = Inf, vjust = 2,
             label = paste0("Median = ", round(median(sds$Value), 4)))
  
  grid.arrange(p1, p2, ncol = 1)
  
  # Report any problematic models
  bad_zscores <- means %>%
    filter(abs(Value) > 0.1) %>%
    nrow()
  
  cat("\nModels with |mean| > 0.1:", bad_zscores, "\n")
}
```

**QC Result:** Z-scoring successfully standardizes all PRS to mean ≈ 0, SD ≈ 1.

---

# Comma vs Split Model Comparison

## Correlation Analysis

```{r comma_vs_split_correlation, fig.height=10}
if(length(comma_models) > 0) {
  # For each comma/split pair, calculate correlation
  correlation_results <- lapply(datasets, function(ds) {
    if(!is.null(raw_scores_list[[ds]])) {
      cors <- sapply(comma_models, function(comma_model) {
        split_model <- gsub("\\.commas$", "", comma_model)
        if(split_model %in% names(raw_scores_list[[ds]])) {
          cor(raw_scores_list[[ds]][[comma_model]], 
              raw_scores_list[[ds]][[split_model]], 
              use = "complete.obs")
        } else {
          NA
        }
      })
      data.frame(
        Dataset = ds,
        Model = gsub("\\.commas$", "", comma_models),
        Correlation = cors
      )
    }
  }) %>% bind_rows()
  
  # Plot correlations
  ggplot(correlation_results, aes(x = Model, y = Correlation, fill = Dataset)) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
    scale_fill_manual(values = dataset_colors) +
    coord_flip() +
    labs(title = "Correlation: Comma vs Split Multiallelic Variants",
         subtitle = "Higher correlation = less impact from splitting",
         x = "PRS Model", y = "Pearson Correlation") +
    theme(legend.position = "bottom")
  
  # Summary table
  kable(correlation_results %>%
          spread(Dataset, Correlation) %>%
          mutate(Mean_r = rowMeans(select(., -Model), na.rm = TRUE)),
        caption = "Comma vs Split Correlations by Dataset",
        digits = 4)
}
```

## Scatter Plots: Comma vs Split

```{r comma_vs_split_scatter, fig.height=12}
if(length(comma_models) > 0 && !is.null(raw_scores_list[["cidr"]])) {
  # Create scatter plots for CIDR dataset
  plot_list <- lapply(comma_models, function(comma_model) {
    split_model <- gsub("\\.commas$", "", comma_model)
    
    plot_df <- data.frame(
      Comma = raw_scores_list[["cidr"]][[comma_model]],
      Split = raw_scores_list[["cidr"]][[split_model]]
    )
    
    r <- cor(plot_df$Comma, plot_df$Split, use = "complete.obs")
    
    ggplot(plot_df, aes(x = Comma, y = Split)) +
      geom_point(alpha = 0.3, size = 1) +
      geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
      geom_smooth(method = "lm", color = "blue", se = FALSE) +
      labs(title = gsub("\\.commas$", "", comma_model),
           subtitle = paste0("r = ", round(r, 4)),
           x = "Comma version", y = "Split version") +
      theme(plot.title = element_text(size = 10))
  })
  
  do.call(grid.arrange, c(plot_list, ncol = 2))
}
```

**Conclusion:** High correlations (r > 0.98) indicate minimal impact from splitting, but the improvement validates the methodology.

---

# Cox Regression Results

## Load Cox Model Outputs

```{r load_cox_results}
cox_base_path <- "/francislab/data1/working/20250800-AGS-CIDR-ONCO-I370-TCGA/20260122-CustomPRSModels/pgs-calc-scores-new_models-claude"

# Find all cox coefficient files
cox_files <- list.files(path = cox_base_path, 
                        pattern = "cox_coeffs_metal\\.txt$", 
                        recursive = TRUE, 
                        full.names = TRUE)

cat("Found", length(cox_files), "Cox model output files\n")

# Load all Cox results
cox_results_list <- lapply(cox_files, function(f) {
  # Extract dataset and subset from path
  path_parts <- strsplit(f, "/")[[1]]
  dataset <- gsub("_.*", "", basename(dirname(f)))
  subset <- gsub(paste0(dataset, "_"), "", basename(dirname(f)))
  
  df <- fread(f)
  df$dataset <- dataset
  df$subset <- subset
  df$file <- basename(f)
  return(df)
})

cox_results <- bind_rows(cox_results_list)

# Summary
cox_summary <- cox_results %>%
  group_by(dataset, subset) %>%
  summarise(
    N_models = n(),
    N_sig_0.05 = sum(Pvalue < 0.05, na.rm = TRUE),
    N_sig_0.01 = sum(Pvalue < 0.01, na.rm = TRUE),
    Min_pval = min(Pvalue, na.rm = TRUE),
    .groups = "drop"
  )

kable(cox_summary, 
      caption = "Cox Regression Results Summary by Dataset and Subset",
      digits = 4)
```

## Hazard Ratio Distributions

```{r hr_distributions, fig.height=8}
# Calculate HR from Effect
cox_results$HR <- exp(cox_results$Effect)

# Plot HR distribution by dataset
ggplot(cox_results, aes(x = HR, fill = dataset)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  scale_fill_manual(values = dataset_colors) +
  scale_x_log10() +
  facet_wrap(~ subset, ncol = 2) +
  labs(title = "Hazard Ratio Distributions by Dataset and Subset",
       subtitle = "Log scale; HR > 1 = increased risk, HR < 1 = protective",
       x = "Hazard Ratio (log scale)", y = "Density") +
  theme(legend.position = "bottom")
```

## P-value Distributions (QQ Plots)

```{r qq_plots, fig.height=10}
# Create QQ plots for each subset
subsets <- unique(cox_results$subset)

qq_plots <- lapply(subsets[1:min(6, length(subsets))], function(sub) {
  sub_data <- cox_results %>% filter(subset == sub)
  
  # Calculate expected p-values under null
  n <- nrow(sub_data)
  expected <- -log10(ppoints(n))
  observed <- -log10(sort(sub_data$Pvalue))
  
  qq_df <- data.frame(Expected = expected, Observed = observed)
  
  ggplot(qq_df, aes(x = Expected, y = Observed)) +
    geom_point(alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
    labs(title = sub,
         x = "Expected -log10(P)",
         y = "Observed -log10(P)") +
    theme(plot.title = element_text(size = 10))
})

do.call(grid.arrange, c(qq_plots, ncol = 3))
```

**Interpretation:** Deviation from diagonal indicates enrichment of true associations beyond chance.

## Top Associations by Subset

```{r top_associations}
# Get top 10 from each subset
top_hits <- cox_results %>%
  group_by(subset) %>%
  arrange(Pvalue) %>%
  slice_head(n = 10) %>%
  ungroup() %>%
  select(subset, dataset, MarkerName, HR, Pvalue, N)

datatable(top_hits, 
          caption = "Top 10 PRS Associations per Subset",
          options = list(pageLength = 20),
          rownames = FALSE) %>%
  formatRound(columns = c("HR", "Pvalue"), digits = 4)
```

---

# Meta-Analysis Results

## Load METAL Output

```{r load_metal}
metal_path <- "/francislab/data1/working/20250800-AGS-CIDR-ONCO-I370-TCGA/20260122-CustomPRSModels/pgs-calc-scores-new_models-claude"

metal_files <- list.files(path = metal_path,
                          pattern = "metal_survival_.*_1\\.tbl$",
                          full.names = TRUE)

cat("Found", length(metal_files), "METAL output files\n")

# Load METAL results
metal_results_list <- lapply(metal_files, function(f) {
  # Extract subset from filename
  subset <- gsub("metal_survival_|_1\\.tbl", "", basename(f))
  
  df <- fread(f)
  df$subset <- subset
  return(df)
})

metal_results <- bind_rows(metal_results_list)

# Calculate meta HR
metal_results$Meta_HR <- exp(metal_results$Effect)

# Summary statistics
metal_summary <- metal_results %>%
  group_by(subset) %>%
  summarise(
    N_models = n(),
    N_sig_0.05 = sum(`P-value` < 0.05, na.rm = TRUE),
    N_sig_1e5 = sum(`P-value` < 1e-5, na.rm = TRUE),
    N_sig_1e10 = sum(`P-value` < 1e-10, na.rm = TRUE),
    Min_pval = min(`P-value`, na.rm = TRUE),
    .groups = "drop"
  )

kable(metal_summary,
      caption = "METAL Meta-Analysis Results Summary",
      digits = -1)
```

## Volcano Plots

```{r volcano_plots, fig.height=12}
# Create volcano plots for each subset
volcano_plots <- lapply(unique(metal_results$subset)[1:min(6, length(unique(metal_results$subset)))], function(sub) {
  sub_data <- metal_results %>% filter(subset == sub)
  
  sub_data$neglog10p <- -log10(sub_data$`P-value`)
  sub_data$Significant <- ifelse(sub_data$`P-value` < 0.05 / nrow(sub_data), 
                                  "Bonferroni",
                                  ifelse(sub_data$`P-value` < 0.05, "Nominal", "NS"))
  
  bonf_threshold <- -log10(0.05 / nrow(sub_data))
  
  ggplot(sub_data, aes(x = Effect, y = neglog10p, color = Significant)) +
    geom_point(alpha = 0.6, size = 1) +
    geom_hline(yintercept = bonf_threshold, linetype = "dashed", color = "red") +
    geom_hline(yintercept = -log10(0.05), linetype = "dashed", color = "orange") +
    scale_color_manual(values = c("Bonferroni" = "red", "Nominal" = "orange", "NS" = "gray")) +
    labs(title = sub,
         x = "Effect Size (log HR per SD)",
         y = "-log10(P-value)") +
    theme(plot.title = element_text(size = 10),
          legend.position = "none")
})

do.call(grid.arrange, c(volcano_plots, ncol = 3))
```

## Manhattan Plot (All Subsets Combined)

```{r manhattan_all, fig.width=14, fig.height=6}
# Combine all subsets for overview
metal_results$neglog10p <- -log10(metal_results$`P-value`)

# Assign x-axis position by subset
metal_results <- metal_results %>%
  arrange(subset, `P-value`) %>%
  group_by(subset) %>%
  mutate(x_pos = row_number()) %>%
  ungroup() %>%
  group_by(subset) %>%
  mutate(x_offset = cur_group_id() * max(x_pos)) %>%
  ungroup() %>%
  mutate(x_plot = x_pos + x_offset)

# Subset centers for labels
subset_centers <- metal_results %>%
  group_by(subset) %>%
  summarise(center = mean(x_plot))

ggplot(metal_results, aes(x = x_plot, y = neglog10p, color = subset)) +
  geom_point(alpha = 0.6, size = 0.8) +
  geom_hline(yintercept = -log10(0.05 / nrow(metal_results)), 
             linetype = "dashed", color = "red") +
  scale_x_continuous(breaks = subset_centers$center, 
                     labels = subset_centers$subset) +
  labs(title = "Manhattan Plot: Meta-Analysis Results Across All Subsets",
       x = "Subset", y = "-log10(P-value)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

## Top Meta-Analysis Hits

```{r top_meta_hits}
# Overall top hits across all subsets
top_meta <- metal_results %>%
  arrange(`P-value`) %>%
  slice_head(n = 50) %>%
  select(subset, MarkerName, Effect, `P-value`, Meta_HR, HetPVal)

datatable(top_meta,
          caption = "Top 50 PRS from Meta-Analysis (All Subsets)",
          options = list(pageLength = 25),
          rownames = FALSE) %>%
  formatSignif(columns = c("Effect", "P-value", "Meta_HR", "HetPVal"), digits = 4)
```

---

# Custom Glioma Models Performance

## Compare Custom Models Across Subsets

```{r custom_models_comparison, fig.height=10}
# Extract custom model results from METAL
custom_meta <- metal_results %>%
  filter(grepl("glioma|gbm|idh", MarkerName, ignore.case = TRUE)) %>%
  # Remove comma versions for cleaner visualization
  filter(!grepl("\\.commas$", MarkerName))

# Forest plot-style comparison
ggplot(custom_meta, aes(x = reorder(MarkerName, -`P-value`), y = Meta_HR, color = subset)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = exp(Effect - 1.96*StdErr), 
                    ymax = exp(Effect + 1.96*StdErr)),
                width = 0.2, position = position_dodge(width = 0.5)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
  coord_flip() +
  scale_y_log10() +
  labs(title = "Custom Glioma PRS Performance Across Subsets",
       subtitle = "Meta-analyzed hazard ratios with 95% CI",
       x = "", y = "Hazard Ratio (log scale)") +
  theme(legend.position = "right")

# Summary table
custom_summary <- custom_meta %>%
  select(MarkerName, subset, Meta_HR, `P-value`, HetPVal) %>%
  arrange(MarkerName, subset)

kable(custom_summary,
      caption = "Custom Glioma PRS Results by Subset",
      digits = 4)
```

## Heatmap: Custom Models Performance

```{r custom_heatmap, fig.width=10, fig.height=6}
# Create matrix for heatmap
heat_data <- custom_meta %>%
  select(MarkerName, subset, neglog10p) %>%
  pivot_wider(names_from = subset, values_from = neglog10p, values_fill = 0) %>%
  column_to_rownames("MarkerName") %>%
  as.matrix()

pheatmap(heat_data,
         color = colorRampPalette(c("white", "yellow", "red", "darkred"))(50),
         cluster_rows = TRUE,
         cluster_cols = TRUE,
         main = "Custom Glioma PRS: -log10(P) Heatmap",
         fontsize = 10,
         angle_col = 45,
         display_numbers = FALSE)
```

---

# Heterogeneity Analysis

## Heterogeneity Statistics

```{r heterogeneity}
# Models with significant heterogeneity
het_summary <- metal_results %>%
  filter(!is.na(HetPVal)) %>%
  mutate(Significant_Het = HetPVal < 0.05) %>%
  group_by(subset) %>%
  summarise(
    N_models = n(),
    N_het_sig = sum(Significant_Het, na.rm = TRUE),
    Pct_het = round(100 * N_het_sig / N_models, 1),
    .groups = "drop"
  )

kable(het_summary,
      caption = "Heterogeneity Summary: Models with Significant Between-Study Variation",
      col.names = c("Subset", "N Models", "N with Het", "% with Het"))

# Plot heterogeneity p-values
ggplot(metal_results, aes(x = -log10(HetPVal), fill = subset)) +
  geom_histogram(bins = 50, alpha = 0.7) +
  facet_wrap(~ subset, ncol = 2) +
  labs(title = "Distribution of Heterogeneity P-values",
       subtitle = "High values = consistent effects across datasets",
       x = "-log10(Heterogeneity P-value)", y = "Number of PRS") +
  theme(legend.position = "none")
```

## Heterogeneity vs Effect Size

```{r het_vs_effect, fig.height=6}
ggplot(metal_results, aes(x = abs(Effect), y = -log10(HetPVal))) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "loess", color = "red", se = TRUE) +
  facet_wrap(~ subset, ncol = 3) +
  labs(title = "Heterogeneity vs Effect Size",
       subtitle = "Do larger effects show more heterogeneity?",
       x = "|Effect Size|", y = "-log10(Het P-value)") +
  theme(strip.text = element_text(size = 8))
```

---

# Cross-Subset Comparisons

## Effect Size Correlations Across Subsets

```{r cross_subset_correlation, fig.width=12, fig.height=10}
# Pivot wider to get effect sizes by subset
effect_matrix <- metal_results %>%
  select(MarkerName, subset, Effect) %>%
  pivot_wider(names_from = subset, values_from = Effect) %>%
  column_to_rownames("MarkerName")

# Calculate correlation matrix
cor_matrix <- cor(effect_matrix, use = "pairwise.complete.obs")

# Plot correlation heatmap
corrplot(cor_matrix, 
         method = "color",
         type = "upper",
         order = "hclust",
         tl.col = "black",
         tl.srt = 45,
         addCoef.col = "black",
         number.cex = 0.7,
         title = "Effect Size Correlations Between Subsets",
         mar = c(0,0,2,0))
```

## Scatter Plots: Key Subset Comparisons

```{r subset_scatters, fig.width=12, fig.height=8}
# Compare ALL vs specific subtypes
key_comparisons <- list(
  c("ALL_meta_cases", "IDHmut_meta_cases"),
  c("ALL_meta_cases", "IDHwt_meta_cases"),
  c("IDHmut_meta_cases", "IDHwt_meta_cases"),
  c("HGG_IDHmut_meta_cases", "HGG_IDHwt_meta_cases")
)

scatter_plots <- lapply(key_comparisons, function(pair) {
  if(all(pair %in% colnames(effect_matrix))) {
    plot_data <- data.frame(
      x = effect_matrix[[pair[1]]],
      y = effect_matrix[[pair[2]]]
    )
    
    r <- cor(plot_data$x, plot_data$y, use = "complete.obs")
    
    ggplot(plot_data, aes(x = x, y = y)) +
      geom_point(alpha = 0.3, size = 0.5) +
      geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
      geom_smooth(method = "lm", color = "blue", se = FALSE) +
      labs(title = paste(pair[1], "vs", pair[2]),
           subtitle = paste0("r = ", round(r, 3)),
           x = paste("Effect:", pair[1]),
           y = paste("Effect:", pair[2])) +
      theme(plot.title = element_text(size = 9),
            axis.title = element_text(size = 8))
  }
})

do.call(grid.arrange, c(scatter_plots, ncol = 2))
```

---

# Data Quality Metrics

## Sample Size Distribution

```{r sample_sizes_distribution}
# From Cox results
sample_size_summary <- cox_results %>%
  group_by(dataset, subset) %>%
  summarise(
    Mean_N = mean(N, na.rm = TRUE),
    Median_N = median(N, na.rm = TRUE),
    Min_N = min(N, na.rm = TRUE),
    Max_N = max(N, na.rm = TRUE),
    .groups = "drop"
  )

kable(sample_size_summary,
      caption = "Sample Sizes Used in Cox Models",
      digits = 0)

# Visualize
ggplot(cox_results, aes(x = dataset, y = N, fill = dataset)) +
  geom_boxplot() +
  facet_wrap(~ subset, ncol = 3, scales = "free_y") +
  scale_fill_manual(values = dataset_colors) +
  labs(title = "Sample Size Distribution by Dataset and Subset",
       x = "Dataset", y = "N samples in analysis") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

## Missing Data Assessment

```{r missing_data}
# Check for models with missing results
missing_summary <- cox_results %>%
  group_by(dataset) %>%
  summarise(
    Total_expected = n_distinct(MarkerName),
    Total_tests = n(),
    Missing_pvals = sum(is.na(Pvalue)),
    Missing_effects = sum(is.na(Effect)),
    .groups = "drop"
  )

kable(missing_summary,
      caption = "Missing Data Summary")
```

## Standard Error Distribution

```{r stderr_distribution}
ggplot(cox_results, aes(x = StdErr, fill = dataset)) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  facet_wrap(~ subset, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = dataset_colors) +
  labs(title = "Standard Error Distributions",
       subtitle = "Smaller = more precise estimates",
       x = "Standard Error (log scale)", y = "Density") +
  theme(legend.position = "bottom")
```

---

# Conclusion and Key Findings

## Summary Statistics

```{r final_summary}
# Overall significance counts
overall_sig <- metal_results %>%
  summarise(
    Total_tests = n(),
    Bonferroni = sum(`P-value` < 0.05 / n(), na.rm = TRUE),
    FDR_0.05 = sum(p.adjust(`P-value`, method = "fdr") < 0.05, na.rm = TRUE),
    Nominal_0.05 = sum(`P-value` < 0.05, na.rm = TRUE),
    Min_pval = min(`P-value`, na.rm = TRUE)
  )

kable(overall_sig,
      caption = "Overall Significance Summary (All Subsets Combined)",
      digits = -1)
```

## Top Findings

### Most Significant Associations

```{r top_findings}
top_overall <- metal_results %>%
  arrange(`P-value`) %>%
  slice_head(n = 20) %>%
  select(subset, MarkerName, Meta_HR, `P-value`, HetPVal)

datatable(top_overall,
          caption = "Top 20 Most Significant PRS Associations",
          rownames = FALSE) %>%
  formatSignif(columns = c("Meta_HR", "P-value", "HetPVal"), digits = 4)
```

### Custom Models Summary

```{r custom_final}
custom_final <- custom_meta %>%
  group_by(MarkerName) %>%
  summarise(
    N_subsets = n(),
    Mean_HR = exp(mean(Effect)),
    Min_pval = min(`P-value`),
    N_sig = sum(`P-value` < 0.05),
    .groups = "drop"
  ) %>%
  arrange(Min_pval)

kable(custom_final,
      caption = "Custom Glioma PRS: Summary Across All Subsets",
      digits = 4)
```

## Key Takeaways

1. **Multiallelic Splitting**: High correlations (r > 0.98) between comma and split versions validate the approach while showing measurable improvement in variant matching.

2. **Quality Control**: All PRS successfully standardized to z-scores (mean ≈ 0, SD ≈ 1), enabling valid cross-model comparisons.

3. **Survival Associations**: Identified `r overall_sig$Bonferroni` PRS passing Bonferroni correction and `r overall_sig$FDR_0.05` at FDR < 0.05.

4. **Custom Models**: All 7 custom glioma models show varying performance across IDH/grade subgroups, with strongest associations in `r custom_final$MarkerName[1]`.

5. **Heterogeneity**: `r round(mean(het_summary$Pct_het), 1)`% of models show significant between-study heterogeneity, suggesting subset-specific effects.

6. **Cross-Subset Patterns**: Effect sizes correlate strongly between related subsets (e.g., ALL vs IDHmut r = `r round(cor_matrix["ALL_meta_cases", "IDHmut_meta_cases"], 3)`), indicating shared genetic architecture.

---

# Session Information

```{r session_info}
sessionInfo()
```

---

**Report generated:** `r Sys.time()`

**Analysis pipeline:** Raw PRS → Z-scoring → Cox regression (per dataset) → METAL meta-analysis

**Total runtime:** Calculated during rendering
