

Trim adapters with quality
Filter out read pairs of inequal length

./pre_process.bash



01_R1
64147452
128294904


No real reason to separate pre processing and processing here.
Not demultiplexing.
Actually, need to evaluate the read lengths and stddev for kallisto



echo "zcat /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/??_R?.fastq.gz | paste - - - - | cut -f 2 | awk '{ l=length; sum+=l; sumsq+=(l)^2; print \"Avg:\",sum/NR,\"\tStddev:\t\"sqrt((sumsq-sum^2/NR)/NR)}' > trimmed.avg_length.ssstdev.txt" | qsub -N length.stdev -l nodes=1:ppn=4 -l vmem=8gb

Not sure how precise this needs to be.
Been running for over 4 hours.
Just fluctuates around 144 / 22

Avg: 145.11 	Stddev:	20.175

Should these numbers be sample specific???




Dark Seq
miRNA Seq

./process.bash




./merge_jellyfish.py -o 13mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.13mers.jellyfish2.csv.gz

./merge_jellyfish.py -o 15mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.15mers.jellyfish2.csv.gz

./merge_jellyfish.py -o 17mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.17mers.jellyfish2.csv.gz

./merge_jellyfish.py -o 21mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.21mers.jellyfish2.csv.gz






Aggregation

./post_process.bash






sqlite3

create table mercounts ( mer varchar(13), c01 int default 0, c02 int default 0 )

.schema mercounts
CREATE TABLE mercounts ( mer varchar(13), c01 int default 0, c02 int default 0 );

.q

sqlite3 mydata.db 'CREATE TABLE mercounts ( mer varchar(13), c01 int default 0, c02 int default 0 )'



date=$( date "+%Y%m%d%H%M%S" )
dir=/francislab/data1/working/20191008_Stanford71/20200211-rerun
qsub -N lengths -l nodes=1:ppn=4 -l vmem=8gb \
	-o ${dir}/trimmed_length_unpaired_read_lengths.${date}.out.txt \
	-e ${dir}/trimmed_length_unpaired_read_lengths.${date}.err.txt \
	${dir}/read_lengths.bash


histogram







aws --profile gwendt s3 sync --exclude \* --include \?\?.h38au.bowtie2-e2e.unmapped.21mers.dsk.txt.gz ./trimmed/length/unpaired/ s3://viral-identification/20191008_Stanford71/



for s in $( seq -w 77 ) ; do
echo $s
mkdir trimmed/length/unpaired/${s}.h38au.bowtie2-e2e.unmapped.21mers.dsk
mv trimmed/length/unpaired/${s}.h38au.bowtie2-e2e.unmapped.21mers.dsk-??????.txt.gz trimmed/length/unpaired/${s}.h38au.bowtie2-e2e.unmapped.21mers.dsk/
done





ll /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/h38au.bowtie2-e2e.unmapped.21mers-??????.dsk-matrix.csv.gz | wc -l
4096





for f in trimmed/length/unpaired/*lt30bp.fastq.gz.read_count.txt ; do b=$(basename $f .lt30bp.fastq.gz.read_count.txt); c=$( cat $f ); echo -e $b"\t"$c; done > lt30bp_read_counts.txt








Investigate read lengths during preprocessing.

DIR=/francislab/data1/working/20191008_Stanford71/20200211-rerun
for f in ${DIR}/trimmed/*fastq.gz ${DIR}/trimmed/length/*fastq.gz ; do
echo read_length_hist.bash $f
done | qsub -N 20191008work -l nodes=1:ppn=8 -l vmem=16gb \
-j oe -o ${DIR}/read_length_hist.bash.out 

Then sum up hist data counts. DON'T RE-READ RAW FILES. This is fast.

DIR=/francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed
zcat ${DIR}/*_R?.fastq.gz.length_hist.csv.gz | awk '{c[$2]+=$1}END{for(n in c){print c[n],n}}' | sort -k2n | gzip > ${DIR}/all_R.length_hist.csv.gz
DIR=/francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length
zcat ${DIR}/*_R?.fastq.gz.length_hist.csv.gz | awk '{c[$2]+=$1}END{for(n in c){print c[n],n}}' | sort -k2n | gzip > ${DIR}/all_R.length_hist.csv.gz


read_length_hist_to_png.py trimmed/all_R.length_hist.csv.gz
read_length_hist_to_png.py trimmed/length/all_R.length_hist.csv.gz



