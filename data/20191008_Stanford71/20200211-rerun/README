

Trim adapters with quality
Filter out read pairs of inequal length

./pre_process.bash



01_R1
64147452
128294904


No real reason to separate pre processing and processing here.
Not demultiplexing.
Actually, need to evaluate the read lengths and stddev for kallisto



echo "zcat /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/??_R?.fastq.gz | paste - - - - | cut -f 2 | awk '{ l=length; sum+=l; sumsq+=(l)^2; print \"Avg:\",sum/NR,\"\tStddev:\t\"sqrt((sumsq-sum^2/NR)/NR)}' > trimmed.avg_length.ssstdev.txt" | qsub -N length.stdev -l nodes=1:ppn=4 -l vmem=8gb

Not sure how precise this needs to be.
Been running for over 4 hours.
Just fluctuates around 144 / 22

Avg: 145.11 	Stddev:	20.175

Should these numbers be sample specific???




Dark Seq
miRNA Seq

./process.bash




./merge_jellyfish.py -o 13mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.13mers.jellyfish2.csv.gz

./merge_jellyfish.py -o 15mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.15mers.jellyfish2.csv.gz

./merge_jellyfish.py -o 17mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.17mers.jellyfish2.csv.gz

./merge_jellyfish.py -o 21mers-matrix.csv.gz /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/??.h38au.bowtie2-e2e.unmapped.21mers.jellyfish2.csv.gz






Aggregation

./post_process.bash






sqlite3

create table mercounts ( mer varchar(13), c01 int default 0, c02 int default 0 )

.schema mercounts
CREATE TABLE mercounts ( mer varchar(13), c01 int default 0, c02 int default 0 );

.q

sqlite3 mydata.db 'CREATE TABLE mercounts ( mer varchar(13), c01 int default 0, c02 int default 0 )'



date=$( date "+%Y%m%d%H%M%S" )
dir=/francislab/data1/working/20191008_Stanford71/20200211-rerun
qsub -N lengths -l nodes=1:ppn=4 -l vmem=8gb \
	-o ${dir}/trimmed_length_unpaired_read_lengths.${date}.out.txt \
	-e ${dir}/trimmed_length_unpaired_read_lengths.${date}.err.txt \
	${dir}/read_lengths.bash


histogram







aws --profile gwendt s3 sync --exclude \* --include \?\?.h38au.bowtie2-e2e.unmapped.21mers.dsk.txt.gz ./trimmed/length/unpaired/ s3://viral-identification/20191008_Stanford71/



for s in $( seq -w 77 ) ; do
echo $s
mkdir trimmed/length/unpaired/${s}.h38au.bowtie2-e2e.unmapped.21mers.dsk
mv trimmed/length/unpaired/${s}.h38au.bowtie2-e2e.unmapped.21mers.dsk-??????.txt.gz trimmed/length/unpaired/${s}.h38au.bowtie2-e2e.unmapped.21mers.dsk/
done





ll /francislab/data1/working/20191008_Stanford71/20200211-rerun/trimmed/length/unpaired/h38au.bowtie2-e2e.unmapped.21mers-??????.dsk-matrix.csv.gz | wc -l
4096





for f in trimmed/length/unpaired/*lt30bp.fastq.gz.read_count.txt ; do b=$(basename $f .lt30bp.fastq.gz.read_count.txt); c=$( cat $f ); echo -e $b"\t"$c; done > lt30bp_read_counts.txt


