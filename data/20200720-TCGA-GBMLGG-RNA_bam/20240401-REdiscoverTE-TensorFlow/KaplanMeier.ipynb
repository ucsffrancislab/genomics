{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb1f172",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/predicting-individual-survival-curves-with-keras-abb1f1f051f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cef0e7",
   "metadata": {},
   "source": [
    "# Predicting individual survival curves with Keras\n",
    "\n",
    "A Deep Learning adaptation of the Kaplan-Meier estimator for customer lifetime value models\n",
    "\n",
    "TL;DR Survival analysis models are widely used in different areas ranging from medicine to e-commerce. There is increasing attention on how to develop individual survival functions rather than population ones, mainly with the use of Deep Learning frameworks. This post introduces a Deep Learning adaptation of one of the most common non-parametric approaches for population survival analysis, the Kaplan-Meier estimator.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Both in research and the industry, there is an increasing interest in predicting individual survival functions, i.e. a survival probability function at any given time. Most of the existing methodologies for this task are either parametric or semi-parametric, while very few remain strictly non-parametric.\n",
    "\n",
    "Some PyTorch implementations of the most popular models based on Deep Learning can be found in the pycox library, while scikit-survival and XGBoost provide other Machine Learning alternatives, like Random Forests and Gradient Boosting, to Survival regression.\n",
    "\n",
    "We introduce an adaptation of one of the most widely known non-parametric survival analysis methods, the Kaplan-Meier estimator, to predict individual survival functions. We achieve this with a Deep Learning variation of the Multi-Task Logistic Regression (MTLR) and N-MTLR. The main difference of our model is that censored data is manipulated using sample weights and the model’s output at each time period is a multiplication of the previous period’s output and a sigmoid layer.\n",
    "\n",
    "## Kaplan-Meier estimate\n",
    "\n",
    "Let be S(t) the probability to live at least t time units, i.e. the survival function:\n",
    "\n",
    "\n",
    "Which by conditional probabilities it is also:\n",
    "\n",
    "\n",
    "\n",
    "Where\n",
    "\n",
    "\n",
    "And its estimator is given by:\n",
    "\n",
    "\n",
    "\n",
    "Putting it into words, the KM estimator at time t is the KM estimator at time t-1 multiplied by the proportion of individuals that haven’t died during time t among those who are known to have survived up to time t.\n",
    "\n",
    "## Deep Learning adaptation\n",
    "\n",
    "Our approach is quite simple:\n",
    "\n",
    "\n",
    "* We represent the above recursion with a multi-output feed-forward neural network in which each output is the previous output multiplied by a sigmoid layer that represents the probability q(t).\n",
    "\n",
    "* Censored data is manipulated using sample weights: for each output t, the sample weight is 1 if the individual’s starting date was at least t time periods ago, and 0 otherwise.\n",
    "\n",
    "\n",
    "How does the code look like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c0dff67",
   "metadata": {},
   "source": [
    "def build_model(\n",
    "    self,\n",
    "    input_shape: int,\n",
    "    hidden_units: List[int],\n",
    "    dropout: Optional[float] = None,\n",
    "    activation: Optional[str] = None,\n",
    "    kernel_regularizer: Optional[str] = None,\n",
    "    kernel_constraint: bool = False,\n",
    "    noise: Optional[float] = None,\n",
    "    normalization: bool = False,\n",
    "):\n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    x = inputs\n",
    "    for units in hidden_units:\n",
    "        x = Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            kernel_constraint=UnitNorm() if kernel_constraint else None,\n",
    "        )(x)\n",
    "        x = GaussianNoise(noise)(x) if noise else x\n",
    "        x = BatchNormalization()(x) if normalization else x\n",
    "        x = Dropout(dropout)(x) if dropout else x\n",
    "    outputs = []\n",
    "    for period in range(self._periods):\n",
    "        if period == 0:\n",
    "            o = Dense(\n",
    "                1,\n",
    "                activation=\"sigmoid\",\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                kernel_constraint=UnitNorm() if kernel_constraint else None,\n",
    "            )(x)\n",
    "            outputs.append(o)\n",
    "            continue\n",
    "        o = Dense(\n",
    "            1,\n",
    "            activation=\"sigmoid\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            kernel_constraint=UnitNorm() if kernel_constraint else None,\n",
    "        )(x)\n",
    "        o = Multiply()([o, outputs[period - 1]])\n",
    "        outputs.append(o)\n",
    "    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def fit(\n",
    "    self,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: List[np.ndarray],\n",
    "    w_train: List[np.ndarray],\n",
    "    validation_data: Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]],\n",
    "    epochs: Optional[int] = 100,\n",
    "    batch_size: Optional[int] = 256,\n",
    "    patience: Optional[int] = 10,\n",
    "):\n",
    "    self.model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\")\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=patience\n",
    "    )\n",
    "    self.model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        sample_weight=w_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        callbacks=[callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05e470",
   "metadata": {},
   "source": [
    "Of course, the architecture is just a reference. To this model’s fit method should be passed X with shape (n_samples, n_features), y (n_samples, periods), and w (n_samples, periods) as explained above.\n",
    "\n",
    "## Summary\n",
    "\n",
    "There are several methods to fit a survival regression model, each with its benefits and drawbacks. In this article, I proposed a very straightforward method that takes advantage of Tensorflow flexibility to use a feed-forward Neural Network to produce individual survival curves without relying on strong assumptions and that is conceptually an adaptation of one of the most common survival analysis models: the Kaplan-Meier estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d58a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198707df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 15:09:49.209805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K \n",
    "\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "#from typing import Multiply\n",
    "#from typing import UnitNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6481d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TFModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        #self, model, model_dir, train_dataset, eval_dataset,\n",
    "        #         learning_rate, num_epochs):\n",
    "        #self.num_epochs = num_epochs\n",
    "        #self.model_dir = model_dir\n",
    "        #self.model = model\n",
    "        #self.train_ds = train_dataset\n",
    "        \n",
    "        print(\"Initializing Tensor Flow Model\")\n",
    "        self._periods = 2\n",
    "\n",
    "    def build_model(\n",
    "        self,\n",
    "        input_shape: int,\n",
    "        hidden_units: List[int],\n",
    "        dropout: Optional[float] = None,\n",
    "        activation: Optional[str] = None,\n",
    "        kernel_regularizer: Optional[str] = None,\n",
    "        kernel_constraint: bool = False,\n",
    "        noise: Optional[float] = None,\n",
    "        normalization: bool = False,\n",
    "    ):\n",
    "        K.clear_session()\n",
    "        inputs = tf.keras.Input(shape=(input_shape,))\n",
    "        x = inputs\n",
    "        for units in hidden_units:\n",
    "            x = tf.keras.layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                kernel_constraint=tf.keras.constraints.UnitNorm() \n",
    "                    if kernel_constraint else None,\n",
    "            )(x)\n",
    "            x = tf.keras.layers.GaussianNoise(noise)(x) if noise else x\n",
    "            x = tf.keras.layers.BatchNormalization()(x) if normalization else x\n",
    "            x = tf.keras.layers.Dropout(dropout)(x) if dropout else x\n",
    "        outputs = []\n",
    "        for period in range(self._periods):\n",
    "            if period == 0:\n",
    "                o = tf.keras.layers.Dense(\n",
    "                    1,\n",
    "                    activation=\"sigmoid\",\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_constraint=UnitNorm() if kernel_constraint else None,\n",
    "                )(x)\n",
    "                outputs.append(o)\n",
    "                continue\n",
    "            o = tf.keras.layers.Dense(\n",
    "                1,\n",
    "                activation=\"sigmoid\",\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                kernel_constraint=tf.keras.constraints.UnitNorm() \n",
    "                    if kernel_constraint else None,\n",
    "            )(x)\n",
    "            o = tf.keras.layers.Multiply()([o, outputs[period - 1]])\n",
    "            outputs.append(o)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: List[np.ndarray],\n",
    "        w_train: List[np.ndarray],\n",
    "        validation_data: Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]],\n",
    "        epochs: Optional[int] = 100,\n",
    "        batch_size: Optional[int] = 256,\n",
    "        patience: Optional[int] = 10,\n",
    "    ):\n",
    "        self.model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\")\n",
    "        callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=patience\n",
    "        )\n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            sample_weight=w_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=[callback],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5215ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Tensor Flow Model\n"
     ]
    }
   ],
   "source": [
    "mytfmodel=TFModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be52345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50000)]              0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  6400128   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   8256      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    65        ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    65        ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 1)                    0         ['dense_3[0][0]',             \n",
      "                                                                     'dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6408514 (24.45 MB)\n",
      "Trainable params: 6408514 (24.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mytfmodel.build_model(input_shape=50000,hidden_units=[128,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc25bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mytfmodel.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
